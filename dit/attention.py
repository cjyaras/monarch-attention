from typing import Optional

from diffusers.models.attention import BasicTransformerBlock

from dit.attention_processor import EfficientAttnProcessor
from dit.config import AttentionType, EfficientAttnConfig


class CustomBasicTransformerBlock(BasicTransformerBlock):
    def __init__(
        self,
        efficient_attention_config: EfficientAttnConfig,
        layer_num: int,
        dim: int,
        num_attention_heads: int,
        attention_head_dim: int,
        dropout=0.0,
        cross_attention_dim: Optional[int] = None,
        activation_fn: str = "geglu",
        num_embeds_ada_norm: Optional[int] = None,
        attention_bias: bool = False,
        only_cross_attention: bool = False,
        double_self_attention: bool = False,
        upcast_attention: bool = False,
        norm_elementwise_affine: bool = True,
        norm_type: str = "layer_norm",  # 'layer_norm', 'ada_norm', 'ada_norm_zero', 'ada_norm_single', 'ada_norm_continuous', 'layer_norm_i2vgen'
        norm_eps: float = 1e-5,
        final_dropout: bool = False,
        fused_attention_type: str = "default",
        positional_embeddings: Optional[str] = None,
        num_positional_embeddings: Optional[int] = None,
        ada_norm_continous_conditioning_embedding_dim: Optional[int] = None,
        ada_norm_bias: Optional[int] = None,
        ff_inner_dim: Optional[int] = None,
        ff_bias: bool = True,
        attention_out_bias: bool = True,
    ):

        super().__init__(
            dim,
            num_attention_heads,
            attention_head_dim,
            dropout,
            cross_attention_dim,
            activation_fn,
            num_embeds_ada_norm,
            attention_bias,
            only_cross_attention,
            double_self_attention,
            upcast_attention,
            norm_elementwise_affine,
            norm_type,
            norm_eps,
            final_dropout,
            fused_attention_type,
            positional_embeddings,
            num_positional_embeddings,
            ada_norm_continous_conditioning_embedding_dim,
            ada_norm_bias,
            ff_inner_dim,
            ff_bias,
            attention_out_bias,
        )

        self.attn1.set_processor(
            EfficientAttnProcessor(efficient_attention_config, layer_num)
        )
        if cross_attention_dim is not None or double_self_attention:
            self.attn2.set_processor(
                EfficientAttnProcessor(efficient_attention_config, layer_num)
            )

from typing import Optional

from diffusers.models.attention import BasicTransformerBlock
from dit.attention_processor import EfficientAttnProcessor
from dit.config import EfficientAttnConfig, AttentionType 



class CustomBasicTransformerBlock(BasicTransformerBlock):
    def __init__(
        self,
        efficient_attention_config: EfficientAttnConfig,
        layer_num: int,
        dim: int,
        num_attention_heads: int,
        attention_head_dim: int,
        dropout=0.0,
        cross_attention_dim: Optional[int] = None,
        activation_fn: str = "geglu",
        num_embeds_ada_norm: Optional[int] = None,
        attention_bias: bool = False,
        only_cross_attention: bool = False,
        double_self_attention: bool = False,
        upcast_attention: bool = False,
        norm_elementwise_affine: bool = True,
        norm_type: str = "layer_norm",  # 'layer_norm', 'ada_norm', 'ada_norm_zero', 'ada_norm_single', 'ada_norm_continuous', 'layer_norm_i2vgen'
        norm_eps: float = 1e-5,
        final_dropout: bool = False,
        fused_attention_type: str = "default",
        positional_embeddings: Optional[str] = None,
        num_positional_embeddings: Optional[int] = None,
        ada_norm_continous_conditioning_embedding_dim: Optional[int] = None,
        ada_norm_bias: Optional[int] = None,
        ff_inner_dim: Optional[int] = None,
        ff_bias: bool = True,
        attention_out_bias: bool = True
    ):
        
        super().__init__(
            dim,
            num_attention_heads,
            attention_head_dim,
            dropout,
            cross_attention_dim,
            activation_fn,
            num_embeds_ada_norm,
            attention_bias,
            only_cross_attention,
            double_self_attention,
            upcast_attention,
            norm_elementwise_affine,
            norm_type,
            norm_eps,
            final_dropout,
            fused_attention_type,
            positional_embeddings,
            num_positional_embeddings,
            ada_norm_continous_conditioning_embedding_dim,
            ada_norm_bias,
            ff_inner_dim,
            ff_bias,
            attention_out_bias
        )

        assert efficient_attention_config.efficient_attention_type in [AttentionType.softmax,
                                                                       AttentionType.soba_monarch,
                                                                       AttentionType.linformer,
                                                                       AttentionType.performer,
                                                                       AttentionType.nystromformer,
                                                                       AttentionType.cosformer]
        
        self.attn1.set_processor(EfficientAttnProcessor(efficient_attention_config, layer_num)) 
        if cross_attention_dim is not None or double_self_attention:
            self.attn2.set_processor(EfficientAttnProcessor(efficient_attention_config, layer_num))